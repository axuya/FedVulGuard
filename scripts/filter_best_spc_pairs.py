#!/usr/bin/env python3
"""
ç­›é€‰æœ€ä½³ SPC å¯¹ç”¨äº Bootstrap è®­ç»ƒ
ä¼˜å…ˆé€‰æ‹©ç›¸ä¼¼åº¦é€‚ä¸­ã€æ¼æ´ç±»å‹æ˜ç¡®çš„é«˜è´¨é‡å¯¹
"""

import json
from pathlib import Path
from collections import Counter

def filter_best_pairs(input_file, output_file, target_size=60):
    """
    ç­›é€‰ç­–ç•¥ï¼š
    1. ä¼˜å…ˆé€‰æ‹©æ¼æ´ç±»å‹æ˜ç¡®çš„ï¼ˆé unknownï¼‰
    2. ç›¸ä¼¼åº¦åœ¨ 0.7-0.95 èŒƒå›´ï¼ˆæœ€æœ‰ä»·å€¼ï¼‰
    3. è´¨é‡åˆ†æ•°é«˜
    4. ä¿æŒæ¼æ´ç±»å‹å¹³è¡¡
    """
    
    print("="*70)
    print("ğŸ” Filtering Best SPC Pairs for Bootstrap Training")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        all_pairs = json.load(f)
    
    print(f"\nğŸ“¦ Input: {len(all_pairs)} pairs")
    
    # åˆ†ç±»
    high_value = []    # é«˜ä»·å€¼ï¼šæ˜ç¡®ç±»å‹ + åˆé€‚ç›¸ä¼¼åº¦
    medium_value = []  # ä¸­ç­‰ä»·å€¼ï¼šæ˜ç¡®ç±»å‹ä½†ç›¸ä¼¼åº¦åé«˜
    low_value = []     # ä½ä»·å€¼ï¼šunknown ç±»å‹
    
    for pair in all_pairs:
        vuln_type = pair.get('vulnerability_type', 'unknown')
        similarity = pair.get('similarity', 0)
        quality = pair.get('quality_score', 0)
        
        # è¯„åˆ†
        score = 0
        
        # 1. æ¼æ´ç±»å‹æ˜ç¡® (+30åˆ†)
        if vuln_type != 'unknown':
            score += 30
        
        # 2. ç›¸ä¼¼åº¦ç†æƒ³ (+40åˆ†)
        if 0.70 <= similarity <= 0.85:
            score += 40
        elif 0.85 < similarity <= 0.95:
            score += 20
        elif similarity > 0.95:
            score += 5
        
        # 3. è´¨é‡åˆ†æ•° (+30åˆ†)
        score += quality * 30
        
        pair['_score'] = score
        
        # åˆ†ç±»
        if vuln_type != 'unknown' and 0.70 <= similarity <= 0.90:
            high_value.append(pair)
        elif vuln_type != 'unknown':
            medium_value.append(pair)
        else:
            low_value.append(pair)
    
    print(f"\nğŸ“Š Classification:")
    print(f"   High value:   {len(high_value)} pairs (known type + ideal similarity)")
    print(f"   Medium value: {len(medium_value)} pairs (known type + high similarity)")
    print(f"   Low value:    {len(low_value)} pairs (unknown type)")
    
    # é€‰æ‹©ç­–ç•¥
    selected = []
    
    # 1. ä¼˜å…ˆé€‰æ‰€æœ‰é«˜ä»·å€¼å¯¹
    selected.extend(high_value)
    print(f"\nâœ… Selected all high-value pairs: {len(high_value)}")
    
    # 2. ä»ä¸­ç­‰ä»·å€¼ä¸­è¡¥å……
    remaining = target_size - len(selected)
    if remaining > 0 and medium_value:
        # æŒ‰è¯„åˆ†æ’åº
        medium_value.sort(key=lambda x: x['_score'], reverse=True)
        
        # æŒ‰æ¼æ´ç±»å‹å¹³è¡¡é€‰æ‹©
        vuln_counts = Counter(p['vulnerability_type'] for p in selected)
        
        for pair in medium_value:
            if len(selected) >= target_size:
                break
            
            vtype = pair['vulnerability_type']
            # é¿å…æŸä¸ªç±»å‹è¿‡å¤š
            if vuln_counts[vtype] < target_size // 4:
                selected.append(pair)
                vuln_counts[vtype] += 1
            elif len(selected) < target_size - 10:  # æ¥è¿‘ç›®æ ‡æ—¶æ”¾å®½é™åˆ¶
                selected.append(pair)
                vuln_counts[vtype] += 1
        
        print(f"âœ… Added from medium-value: {len(selected) - len(high_value)}")
    
    # 3. å¦‚æœè¿˜ä¸å¤Ÿï¼Œä»ä½ä»·å€¼ä¸­é€‰æ‹©è´¨é‡æœ€é«˜çš„
    remaining = target_size - len(selected)
    if remaining > 0 and low_value:
        low_value.sort(key=lambda x: x['_score'], reverse=True)
        selected.extend(low_value[:remaining])
        print(f"âœ… Added from low-value: {min(remaining, len(low_value))}")
    
    # é‡æ–°åˆ†é… pair_id
    for i, pair in enumerate(selected):
        pair['pair_id'] = f"filtered_{i:04d}"
        # åˆ é™¤ä¸´æ—¶è¯„åˆ†å­—æ®µ
        if '_score' in pair:
            del pair['_score']
    
    # ç»Ÿè®¡
    print(f"\nğŸ“Š Selected Dataset Statistics:")
    print(f"   Total pairs: {len(selected)}")
    
    vuln_dist = Counter(p['vulnerability_type'] for p in selected)
    print(f"\n   Vulnerability distribution:")
    for vtype, count in vuln_dist.most_common():
        print(f"      {vtype:20s}: {count:3d}")
    
    similarities = [p['similarity'] for p in selected if p.get('similarity')]
    if similarities:
        print(f"\n   Similarity statistics:")
        print(f"      Average: {sum(similarities)/len(similarities):.3f}")
        print(f"      Range: {min(similarities):.3f} - {max(similarities):.3f}")
        
        # åŒºé—´åˆ†å¸ƒ
        ideal_sim = [s for s in similarities if 0.7 <= s <= 0.9]
        print(f"      In ideal range (0.7-0.9): {len(ideal_sim)} ({len(ideal_sim)/len(similarities)*100:.1f}%)")
    
    quality_scores = [p['quality_score'] for p in selected if p.get('quality_score')]
    if quality_scores:
        high_q = sum(1 for q in quality_scores if q >= 1.0)
        print(f"\n   Quality metrics:")
        print(f"      High quality (â‰¥1.0): {high_q} ({high_q/len(selected)*100:.1f}%)")
    
    # ä¿å­˜
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(selected, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… Saved to: {output_path}")
    print(f"   File size: {output_path.stat().st_size / 1024:.1f} KB")
    
    # è´¨é‡è¯„ä¼°
    print("\n" + "="*70)
    print("âœ… Quality Assessment")
    print("="*70)
    
    known_type = len([p for p in selected if p['vulnerability_type'] != 'unknown'])
    print(f"âœ… Known vulnerability types: {known_type}/{len(selected)} ({known_type/len(selected)*100:.0f}%)")
    
    if ideal_sim:
        print(f"âœ… Pairs in ideal similarity range: {len(ideal_sim)}/{len(selected)} ({len(ideal_sim)/len(selected)*100:.0f}%)")
    
    if len(vuln_dist) >= 3:
        print(f"âœ… Good type diversity: {len(vuln_dist)} types")
    
    print("\nğŸ’¡ This filtered dataset is optimized for Bootstrap training!")
    
    return selected


def compare_datasets(original_file, filtered_file):
    """æ¯”è¾ƒåŸå§‹å’Œç­›é€‰åçš„æ•°æ®é›†"""
    
    with open(original_file, 'r') as f:
        original = json.load(f)
    
    with open(filtered_file, 'r') as f:
        filtered = json.load(f)
    
    print("\n" + "="*70)
    print("ğŸ“Š Before vs After Comparison")
    print("="*70)
    
    print(f"\nDataset size:")
    print(f"   Original:  {len(original)} pairs")
    print(f"   Filtered:  {len(filtered)} pairs")
    print(f"   Reduction: {len(original) - len(filtered)} pairs ({(1-len(filtered)/len(original))*100:.0f}%)")
    
    # æ¼æ´ç±»å‹å¯¹æ¯”
    print(f"\nUnknown type proportion:")
    orig_unknown = sum(1 for p in original if p['vulnerability_type'] == 'unknown')
    filt_unknown = sum(1 for p in filtered if p['vulnerability_type'] == 'unknown')
    print(f"   Original:  {orig_unknown}/{len(original)} ({orig_unknown/len(original)*100:.0f}%)")
    print(f"   Filtered:  {filt_unknown}/{len(filtered)} ({filt_unknown/len(filtered)*100:.0f}%)")
    
    # ç›¸ä¼¼åº¦å¯¹æ¯”
    orig_sim = [p['similarity'] for p in original if p.get('similarity')]
    filt_sim = [p['similarity'] for p in filtered if p.get('similarity')]
    
    print(f"\nAverage similarity:")
    print(f"   Original:  {sum(orig_sim)/len(orig_sim):.3f}")
    print(f"   Filtered:  {sum(filt_sim)/len(filt_sim):.3f}")


def main():
    input_file = "data/spc_data/processed/bootstrap_spc_dataset.json"
    output_file = "data/spc_data/processed/bootstrap_filtered_60.json"
    
    # ç­›é€‰
    selected = filter_best_pairs(input_file, output_file, target_size=60)
    
    # å¯¹æ¯”
    compare_datasets(input_file, output_file)
    
    print("\n" + "="*70)
    print("ğŸ‰ Filtering Complete!")
    print("="*70)
    print("\nğŸ“ Next Steps:")
    print("   1. Review the filtered dataset")
    print("   2. [Optional] Manual annotation of 'unknown' types")
    print("   3. Use for Bootstrap SPC detector training")
    print("   4. Proceed to Phase 2 (Multi-graph representation)")


if __name__ == "__main__":
    main()