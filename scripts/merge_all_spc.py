#!/usr/bin/env python3
"""
åˆå¹¶æ‰€æœ‰ SPC æ•°æ®æºå¹¶ç”Ÿæˆæœ€ç»ˆçš„ Bootstrap æ•°æ®é›†
"""

import json
from pathlib import Path
from typing import List, Dict

def load_spc_pairs() -> List[Dict]:
    """åŠ è½½æ‰€æœ‰ SPC æ•°æ®"""
    spc_dir = Path("data/spc_data/raw")
    all_pairs = []
    
    spc_files = [
        'spc_pairs_from_datasets.json',
        'spc_pairs_enhanced.json',
        'spc_pairs_search.json',  # GitHub æœç´¢
        'spc_pairs_repos.json',   # GitHub ä»“åº“
        'spc_pairs_all.json'      # GitHub åˆå¹¶
    ]
    
    sources_found = []
    
    for filename in spc_files:
        filepath = spc_dir / filename
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    pairs = json.load(f)
                    if pairs:
                        all_pairs.extend(pairs)
                        sources_found.append(f"{filename}: {len(pairs)} pairs")
            except Exception as e:
                print(f"âš ï¸  Error loading {filename}: {e}")
    
    print("\nğŸ“¦ Loaded from sources:")
    for source in sources_found:
        print(f"   {source}")
    
    return all_pairs


def deduplicate_pairs(pairs: List[Dict]) -> List[Dict]:
    """å»é‡ï¼ˆåŸºäºä»£ç å“ˆå¸Œï¼‰"""
    seen = set()
    unique = []
    
    for pair in pairs:
        # åˆ›å»ºå”¯ä¸€æ ‡è¯†
        key = f"{pair.get('code_before', '')}||{pair.get('code_after', '')}"
        key_hash = hash(key)
        
        if key_hash not in seen:
            seen.add(key_hash)
            unique.append(pair)
    
    print(f"\nğŸ” Deduplication: {len(pairs)} â†’ {len(unique)} unique pairs")
    return unique


def filter_high_quality(pairs: List[Dict]) -> List[Dict]:
    """ç­›é€‰é«˜è´¨é‡ SPC å¯¹"""
    filtered = []
    
    for pair in pairs:
        # è´¨é‡æ ‡å‡†
        code_before = pair.get('code_before', '')
        code_after = pair.get('code_after', '')
        similarity = pair.get('similarity', 0)
        
        # 1. ä»£ç é•¿åº¦åˆç†
        if len(code_before) < 50 or len(code_after) < 50:
            continue
        
        # 2. ç›¸ä¼¼åº¦åœ¨åˆç†èŒƒå›´
        if similarity and (similarity < 0.6 or similarity > 0.98):
            continue
        
        # 3. æœ‰æ˜ç¡®çš„æ¼æ´ç±»å‹ï¼ˆä¼˜å…ˆï¼‰
        if pair.get('vulnerability_type') != 'unknown':
            pair['quality_score'] = 1.0
        else:
            pair['quality_score'] = 0.5
        
        filtered.append(pair)
    
    # æŒ‰è´¨é‡è¯„åˆ†æ’åº
    filtered.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
    
    print(f"âœ… Quality filtering: {len(pairs)} â†’ {len(filtered)} high-quality pairs")
    return filtered


def create_bootstrap_dataset(pairs: List[Dict], target_size: int = 100):
    """åˆ›å»º Bootstrap æ•°æ®é›†"""
    print(f"\nğŸ¯ Creating Bootstrap dataset (target: {target_size} pairs)...")
    
    # æŒ‰æ¼æ´ç±»å‹åˆ†ç»„
    by_type = {}
    for pair in pairs:
        vtype = pair.get('vulnerability_type', 'unknown')
        if vtype not in by_type:
            by_type[vtype] = []
        by_type[vtype].append(pair)
    
    # ä»æ¯ä¸ªç±»å‹é€‰æ‹©
    bootstrap = []
    per_type = target_size // len(by_type)
    
    for vtype, type_pairs in by_type.items():
        selected = type_pairs[:per_type]
        bootstrap.extend(selected)
        print(f"   {vtype}: {len(selected)} pairs")
    
    # å¦‚æœè¿˜ä¸å¤Ÿï¼Œä»å‰©ä½™ä¸­è¡¥å……
    if len(bootstrap) < target_size:
        remaining = [p for p in pairs if p not in bootstrap]
        bootstrap.extend(remaining[:target_size - len(bootstrap)])
    
    # é‡æ–°åˆ†é… pair_id
    for i, pair in enumerate(bootstrap):
        pair['pair_id'] = f"bootstrap_{i:04d}"
    
    return bootstrap[:target_size]


def generate_statistics(pairs: List[Dict]):
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š Dataset Statistics")
    print("="*60)
    
    print(f"Total pairs: {len(pairs)}")
    
    # æ¼æ´ç±»å‹åˆ†å¸ƒ
    vuln_dist = {}
    for pair in pairs:
        vtype = pair.get('vulnerability_type', 'unknown')
        vuln_dist[vtype] = vuln_dist.get(vtype, 0) + 1
    
    print("\nğŸ”– Vulnerability Distribution:")
    for vtype, count in sorted(vuln_dist.items(), key=lambda x: x[1], reverse=True):
        percentage = count / len(pairs) * 100
        print(f"   {vtype:20s}: {count:3d} ({percentage:5.1f}%)")
    
    # ç›¸ä¼¼åº¦åˆ†å¸ƒ
    similarities = [p.get('similarity', 0) for p in pairs if p.get('similarity')]
    if similarities:
        avg_sim = sum(similarities) / len(similarities)
        print(f"\nğŸ“ Average similarity: {avg_sim:.3f}")
        print(f"   Min: {min(similarities):.3f}")
        print(f"   Max: {max(similarities):.3f}")
    
    # æ•°æ®æ¥æº
    sources = {}
    for pair in pairs:
        method = pair.get('method', pair.get('patch_method', 'unknown'))
        sources[method] = sources.get(method, 0) + 1
    
    print("\nğŸ“ Data Sources:")
    for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
        print(f"   {source}: {count}")


def create_annotation_template(pairs: List[Dict]):
    """åˆ›å»ºæ ‡æ³¨æ¨¡æ¿"""
    template = []
    
    for pair in pairs:
        item = {
            'pair_id': pair.get('pair_id'),
            'vulnerability_type': pair.get('vulnerability_type'),
            'similarity': pair.get('similarity'),
            'code_before_preview': pair.get('code_before', '')[:200] + '...',
            'code_after_preview': pair.get('code_after', '')[:200] + '...',
            'annotation': {
                'is_valid_spc': None,  # äººå·¥æ ‡æ³¨: True/False
                'confirmed_vulnerability_type': None,
                'severity': None,  # low/medium/high/critical
                'quality_rating': None,  # 1-5
                'notes': ''
            }
        }
        template.append(item)
    
    output_path = Path("data/spc_data/annotated/bootstrap_annotation_template.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(template, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“ Annotation template: {output_path}")


def main():
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         SPC Data Merger & Bootstrap Creator             â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # 1. åŠ è½½æ‰€æœ‰æ•°æ®
    all_pairs = load_spc_pairs()
    
    if not all_pairs:
        print("\nâŒ No SPC pairs found! Run the collection scripts first:")
        print("   python src/data_collection/enhanced_spc_builder.py")
        return
    
    # 2. å»é‡
    unique_pairs = deduplicate_pairs(all_pairs)
    
    # 3. è´¨é‡è¿‡æ»¤
    quality_pairs = filter_high_quality(unique_pairs)
    
    # 4. åˆ›å»º Bootstrap æ•°æ®é›†
    bootstrap = create_bootstrap_dataset(quality_pairs, target_size=100)
    
    # 5. ä¿å­˜
    output_dir = Path("data/spc_data/processed")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´æ•°æ®
    with open(output_dir / 'all_spc_pairs.json', 'w', encoding='utf-8') as f:
        json.dump(quality_pairs, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜ Bootstrap æ•°æ®
    with open(output_dir / 'bootstrap_spc_dataset.json', 'w', encoding='utf-8') as f:
        json.dump(bootstrap, f, indent=2, ensure_ascii=False)
    
    # 6. ç»Ÿè®¡
    generate_statistics(bootstrap)
    
    # 7. åˆ›å»ºæ ‡æ³¨æ¨¡æ¿
    create_annotation_template(bootstrap)
    
    print("\n" + "="*60)
    print("âœ… SPC Data Preparation Complete!")
    print("="*60)
    print(f"\nğŸ“ Output files:")
    print(f"   All pairs: data/spc_data/processed/all_spc_pairs.json ({len(quality_pairs)} pairs)")
    print(f"   Bootstrap: data/spc_data/processed/bootstrap_spc_dataset.json ({len(bootstrap)} pairs)")
    print(f"   Annotation template: data/spc_data/annotated/bootstrap_annotation_template.json")
    
    print("\nğŸ’¡ Next Steps:")
    print("   1. Review bootstrap_annotation_template.json")
    print("   2. Manually annotate the pairs")
    print("   3. Select top 50-100 high-quality pairs for Bootstrap phase")
    print("   4. Proceed to Phase 2 (data preprocessing)")


if __name__ == "__main__":
    main()