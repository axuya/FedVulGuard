#!/usr/bin/env python3
"""
å¤„ç† smart-contract-sanctuary å¤§è§„æ¨¡æ•°æ®é›†
"""

import json
import hashlib
from pathlib import Path
from tqdm import tqdm
from collections import Counter
import multiprocessing as mp
import random

class LargeScaleDataProcessor:
    """å¤§è§„æ¨¡æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, 
                 sanctuary_dir="/home/xu/FedVulGuard/data/raw/sanctuary_full",
                 output_dir="data/processed_large_scale"):
        self.sanctuary_dir = Path(sanctuary_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def collect_all_contracts(self):
        """æ”¶é›†æ‰€æœ‰åˆçº¦"""
        print("ğŸ“¦ Step 1: æ”¶é›†æ‰€æœ‰åˆçº¦æ–‡ä»¶")
        
        all_contracts = []
        
        if self.sanctuary_dir.exists():
            print(f"   æ‰«æ sanctuary: {self.sanctuary_dir}")
            for sol_file in tqdm(list(self.sanctuary_dir.rglob("*.sol")), desc="Sanctuary"):
                all_contracts.append({
                    'path': sol_file,
                    'source': 'sanctuary',
                    'chain': self._infer_chain_from_path(sol_file)
                })
        
        print(f"\nâœ… æ”¶é›†åˆ° {len(all_contracts)} ä¸ªåˆçº¦æ–‡ä»¶")
        return all_contracts
    
    def _infer_chain_from_path(self, path):
        """ä»è·¯å¾„æ¨æ–­é“¾"""
        path_str = str(path).lower()
        if 'ethereum' in path_str or 'mainnet' in path_str:
            return 'ethereum'
        elif 'bsc' in path_str or 'binance' in path_str:
            return 'bsc'
        elif 'polygon' in path_str or 'matic' in path_str:
            return 'polygon'
        elif 'avalanche' in path_str or 'avax' in path_str:
            return 'avalanche'
        return 'unknown'
    
    def quality_filter(self, contracts, batch_size=10000):
        """è´¨é‡è¿‡æ»¤"""
        print("\nğŸ” Step 2: è´¨é‡è¿‡æ»¤")
        
        filtered = []
        seen_hashes = set()
        
        for i in tqdm(range(0, len(contracts), batch_size), desc="æ‰¹å¤„ç†"):
            batch = contracts[i:i+batch_size]
            
            with mp.Pool(mp.cpu_count()) as pool:
                results = pool.map(self._process_contract, batch)
            
            for result in results:
                if result and result['code_hash'] not in seen_hashes:
                    seen_hashes.add(result['code_hash'])
                    filtered.append(result)
        
        print(f"âœ… è¿‡æ»¤å: {len(filtered)} ä¸ªåˆçº¦")
        return filtered
    
    def _process_contract(self, contract_info):
        """å¤„ç†å•ä¸ªåˆçº¦"""
        try:
            with open(contract_info['path'], 'r', encoding='utf-8', errors='ignore') as f:
                code = f.read()
            
            if len(code) < 50 or len(code) > 100000:
                return None
            
            if not any(x in code.lower() for x in ['pragma','contract','function']):
                return None
            
            lines = code.count('\n')
            if lines < 5 or lines > 5000:
                return None
            
            code_hash = hashlib.sha256(code.encode()).hexdigest()
            
            return {
                'contract_id': contract_info['path'].stem,
                'code': code,
                'code_hash': code_hash,
                'chain': contract_info['chain'],
                'source': contract_info['source'],
                'sloc': lines
            }
            
        except:
            return None
    
    def _reclassify_unknown(self, unknown_contracts, max_check=50000):
        """é‡æ–°è¯†åˆ« unknown åˆçº¦çš„é“¾"""
        reclassified = {
            'ethereum': [],
            'bsc': [],
            'polygon': [],
            'avalanche': []
        }
        
        check_list = unknown_contracts[:max_check]
        
        for contract in tqdm(check_list, desc="é‡æ–°åˆ†ç±»"):
            try:
                code = contract.get('code', '')
                
                if any(x in code for x in ['pancake', 'PancakeSwap', '0xbb4']):
                    reclassified['bsc'].append(contract)
                elif any(x in code for x in ['polygon', 'matic', '0x0d500']):
                    reclassified['polygon'].append(contract)
                elif any(x in code for x in ['avalanche', 'avax', '0xB31f']):
                    reclassified['avalanche'].append(contract)
                else:
                    reclassified['ethereum'].append(contract)
            except:
                reclassified['ethereum'].append(contract)
        
        return reclassified
    
    def stratified_sampling(self, contracts, target_size=10000, train_ratio=0.7, val_ratio=0.15):
        """åˆ†å±‚æŠ½æ · - å¹³è¡¡é“¾åˆ†å¸ƒ"""
        print(f"\nğŸ“Š Step 3: åˆ†å±‚æŠ½æ · (ç›®æ ‡: {target_size})")
        
        by_chain = {}
        for c in contracts:
            chain = c['chain']
            if chain not in by_chain:
                by_chain[chain] = []
            by_chain[chain].append(c)
        
        print("\nåŸå§‹é“¾åˆ†å¸ƒ:")
        for chain, items in by_chain.items():
            print(f"   {chain}: {len(items)}")
        
        print("\nğŸ” é‡æ–°è¯†åˆ« unknown é“¾...")
        unknown_items = by_chain.get('unknown', [])
        reclassified = self._reclassify_unknown(unknown_items)
        
        for chain, items in reclassified.items():
            if chain not in by_chain:
                by_chain[chain] = []
            by_chain[chain].extend(items)
        by_chain.pop('unknown', None)
        
        print("\né‡æ–°åˆ†ç±»å:")
        for chain, items in by_chain.items():
            print(f"   {chain}: {len(items)}")
        
        sampled = []
        target_chains = ['ethereum', 'bsc', 'polygon', 'avalanche']
        per_chain = target_size // len(target_chains)
        
        print(f"\nç›®æ ‡ï¼šæ¯æ¡é“¾ {per_chain} ä¸ªæ ·æœ¬")
        
        for chain in target_chains:
            items = by_chain.get(chain, [])
            if len(items) < per_chain:
                print(f"   âš ï¸  {chain}: åªæœ‰ {len(items)} ä¸ª")
                sampled.extend(items)
            else:
                random.shuffle(items)
                sampled.extend(items[:per_chain])
                print(f"   âœ… {chain}: é‡‡æ · {per_chain} ä¸ª")
        
        random.shuffle(sampled)
        
        train_size = int(len(sampled) * train_ratio)
        val_size = int(len(sampled) * val_ratio)
        
        train_data = sampled[:train_size]
        val_data = sampled[train_size:train_size+val_size]
        test_data = sampled[train_size+val_size:]
        
        print(f"\nâœ… é‡‡æ ·å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(train_data)}")
        print(f"   éªŒè¯é›†: {len(val_data)}")
        print(f"   æµ‹è¯•é›†: {len(test_data)}")
        
        return {
            'train': train_data,
            'val': val_data,
            'test': test_data,
            'full': sampled
        }
    
    def save_dataset(self, dataset, prefix='large_scale'):
        """ä¿å­˜æ•°æ®é›†"""
        print(f"\nğŸ’¾ Step 4: ä¿å­˜æ•°æ®é›†")
        
        for split, data in dataset.items():
            if split == 'full':
                continue
            
            output_file = self.output_dir / f"{prefix}_{split}.json"
            
            compact_data = []
            for item in data:
                compact_data.append({
                    'contract_id': item['contract_id'],
                    'code_hash': item['code_hash'],
                    'chain': item['chain'],
                    'sloc': item['sloc'],
                })
            
            with open(output_file, 'w') as f:
                json.dump(compact_data, f, indent=2)
            
            print(f"   âœ… {output_file} ({len(compact_data)} æ¡)")
            
            code_dir = self.output_dir / f"{prefix}_{split}_code"
            code_dir.mkdir(exist_ok=True)
            
            for item in tqdm(data, desc=f"ä¿å­˜ {split} ä»£ç "):
                code_file = code_dir / f"{item['contract_id']}.sol"
                with open(code_file, 'w', encoding='utf-8') as f:
                    f.write(item['code'])
        
        print("\nâœ… æ•°æ®é›†ä¿å­˜å®Œæˆ")
    
    def generate_statistics(self, dataset):
        """ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡"""
        print("\n" + "="*70)
        print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡")
        print("="*70)
        
        for split, data in dataset.items():
            if split == 'full':
                continue
            
            print(f"\n{split.upper()}:")
            print(f"   æ ·æœ¬æ•°: {len(data)}")
            
            chain_dist = Counter(item['chain'] for item in data)
            print(f"   é“¾åˆ†å¸ƒ:")
            for chain, count in chain_dist.most_common():
                print(f"      {chain}: {count} ({count/len(data)*100:.1f}%)")
            
            slocs = [item['sloc'] for item in data]
            print(f"   ä»£ç è¡Œæ•°: avg={sum(slocs)/len(slocs):.0f}, min={min(slocs)}, max={max(slocs)}")


def main():
    processor = LargeScaleDataProcessor()
    
    all_contracts = processor.collect_all_contracts()
    
    if len(all_contracts) == 0:
        print("\nâŒ æœªæ‰¾åˆ°åˆçº¦ï¼")
        return
    
    filtered = processor.quality_filter(all_contracts)
    dataset = processor.stratified_sampling(filtered, target_size=10000)
    processor.save_dataset(dataset)
    processor.generate_statistics(dataset)
    
    print("\nâœ… å®Œæˆï¼")


if __name__ == "__main__":
    main()