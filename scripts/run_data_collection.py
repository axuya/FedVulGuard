#!/usr/bin/env python3
"""
æ•°æ®æ”¶é›†ä¸»è„šæœ¬
æŒ‰æ­¥éª¤æ‰§è¡Œ Etherscan å’Œ GitHub æ•°æ®æ”¶é›†
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data_collection.etherscan_crawler import EtherscanCrawler
from src.data_collection.github_spc_crawler import GitHubSPCCrawler
from src.utils.data_utils import (
    extract_contract_addresses_from_datasets,
    save_addresses_list,
    merge_spc_data,
    get_vulnerability_distribution
)
import argparse
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def step1_extract_addresses():
    """æ­¥éª¤1: ä»ç°æœ‰æ•°æ®é›†æå–åœ°å€"""
    print("\n" + "="*60)
    print("STEP 1: Extracting contract addresses from datasets")
    print("="*60)
    
    addresses = extract_contract_addresses_from_datasets()
    
    if addresses:
        save_addresses_list(addresses, "data/contract_addresses.txt")
        print(f"âœ… Extracted {len(addresses)} addresses")
    else:
        print("âš ï¸  No addresses found in datasets")
        print("ğŸ’¡ You can manually add addresses to data/contract_addresses.txt")
    
    return addresses


def step2_crawl_etherscan(use_known_addresses: bool = True):
    """æ­¥éª¤2: çˆ¬å– Etherscan åˆçº¦"""
    print("\n" + "="*60)
    print("STEP 2: Crawling Etherscan contracts")
    print("="*60)
    
    crawler = EtherscanCrawler()
    
    # å†³å®šä½¿ç”¨å“ªäº›åœ°å€
    if use_known_addresses:
        # æ–¹æ¡ˆA: ä½¿ç”¨å·²çŸ¥ DeFi åœ°å€
        print("Using known DeFi contract addresses...")
        addresses = crawler.get_defi_contracts()
    else:
        # æ–¹æ¡ˆB: ä½¿ç”¨ä»æ•°æ®é›†æå–çš„åœ°å€
        addresses_file = Path("data/contract_addresses.txt")
        if addresses_file.exists():
            print(f"Loading addresses from {addresses_file}...")
            with open(addresses_file, 'r') as f:
                addresses = [line.strip() for line in f if line.strip()]
        else:
            print("âŒ No address file found. Run step 1 first or use known addresses.")
            return
    
    print(f"Total addresses to crawl: {len(addresses)}")
    
    if not addresses:
        print("âŒ No addresses to crawl!")
        return
    
    # å¼€å§‹çˆ¬å–
    crawler.crawl_contracts(addresses, save_batch_size=50)
    
    # è¿‡æ»¤
    print("\nFiltering contracts...")
    filtered = crawler.filter_contracts(
        min_size=100,
        max_size=5000
    )
    
    # ç»Ÿè®¡
    print("\nGenerating statistics...")
    stats = crawler.generate_statistics()
    
    print("\n" + "-"*60)
    print("ğŸ“Š Etherscan Crawling Statistics:")
    print(f"  Total contracts: {stats['total_contracts']}")
    print(f"  Average code length: {stats['avg_code_length']:.2f} lines")
    print(f"  Optimization enabled: {stats['optimization_enabled']}")
    print("-"*60)
    
    return filtered


def step3_collect_spc_data(target_pairs: int = 500):
    """æ­¥éª¤3: æ”¶é›† SPC æ•°æ®"""
    print("\n" + "="*60)
    print("STEP 3: Collecting SPC pairs from GitHub")
    print("="*60)
    
    crawler = GitHubSPCCrawler()
    
    # æ–¹æ³•1: å…³é”®è¯æœç´¢
    print("\n3.1 Collecting from keyword search...")
    keyword_pairs = crawler.collect_spc_pairs(max_pairs=target_pairs)
    
    # æ–¹æ³•2: ç›®æ ‡ä»“åº“
    print("\n3.2 Collecting from target repositories...")
    repo_pairs = crawler.collect_from_target_repos()
    
    # åˆå¹¶
    all_pairs = keyword_pairs + repo_pairs
    print(f"\nâœ… Total SPC pairs collected: {len(all_pairs)}")
    
    # ç”Ÿæˆæ ‡æ³¨æ¨¡æ¿
    print("\n3.3 Generating annotation template...")
    crawler.generate_annotation_template(all_pairs)
    
    # ç»Ÿè®¡æ¼æ´åˆ†å¸ƒ
    distribution = get_vulnerability_distribution(all_pairs)
    print("\nğŸ“Š Vulnerability Distribution:")
    for vuln_type, count in sorted(distribution.items(), key=lambda x: x[1], reverse=True):
        print(f"  {vuln_type}: {count}")
    
    return all_pairs


def step4_merge_and_validate():
    """æ­¥éª¤4: åˆå¹¶å’ŒéªŒè¯æ•°æ®"""
    print("\n" + "="*60)
    print("STEP 4: Merging and validating data")
    print("="*60)
    
    # åˆå¹¶ SPC æ•°æ®
    print("Merging SPC data...")
    spc_pairs = merge_spc_data()
    
    print(f"\nâœ… Data collection pipeline completed!")
    print(f"ğŸ“ Check the following directories:")
    print(f"  - Etherscan data: data/etherscan/")
    print(f"  - SPC data: data/spc_data/")
    print(f"  - Logs: logs/")
    
    return spc_pairs


def main():
    parser = argparse.ArgumentParser(description='FedVulGuard Data Collection Pipeline')
    parser.add_argument('--step', type=int, choices=[1, 2, 3, 4], 
                       help='Run specific step (1-4), or run all if not specified')
    parser.add_argument('--etherscan-mode', choices=['known', 'extracted'], default='known',
                       help='Etherscan address source: known DeFi addresses or extracted from datasets')
    parser.add_argument('--spc-pairs', type=int, default=500,
                       help='Target number of SPC pairs to collect')
    
    args = parser.parse_args()
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         FedVulGuard Data Collection Pipeline            â•‘
    â•‘                                                          â•‘
    â•‘  This script will collect:                              â•‘
    â•‘    1. Contract addresses from existing datasets         â•‘
    â•‘    2. Smart contract source code from Etherscan         â•‘
    â•‘    3. SPC (Similar Patched Code) pairs from GitHub      â•‘
    â•‘    4. Merge and validate collected data                 â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    if args.step:
        # è¿è¡Œç‰¹å®šæ­¥éª¤
        if args.step == 1:
            step1_extract_addresses()
        elif args.step == 2:
            step2_crawl_etherscan(use_known_addresses=(args.etherscan_mode == 'known'))
        elif args.step == 3:
            step3_collect_spc_data(target_pairs=args.spc_pairs)
        elif args.step == 4:
            step4_merge_and_validate()
    else:
        # è¿è¡Œå®Œæ•´æµç¨‹
        try:
            addresses = step1_extract_addresses()
            
            input("\nâ¸ï¸  Press Enter to continue to Step 2 (Etherscan crawling)...")
            filtered_contracts = step2_crawl_etherscan(
                use_known_addresses=(args.etherscan_mode == 'known')
            )
            
            input("\nâ¸ï¸  Press Enter to continue to Step 3 (SPC collection)...")
            spc_pairs = step3_collect_spc_data(target_pairs=args.spc_pairs)
            
            input("\nâ¸ï¸  Press Enter to continue to Step 4 (Merge and validate)...")
            step4_merge_and_validate()
            
            print("\n" + "="*60)
            print("ğŸ‰ DATA COLLECTION COMPLETED!")
            print("="*60)
            print("\nğŸ“‹ Next Steps:")
            print("  1. Review the annotation template in data/spc_data/annotated/")
            print("  2. Manually annotate the SPC pairs")
            print("  3. Run data preprocessing (Phase 2)")
            print("  4. Build multi-graph representations (Phase 3)")
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸  Process interrupted by user")
            sys.exit(1)
        except Exception as e:
            logger.error(f"Error during data collection: {e}", exc_info=True)
            sys.exit(1)


if __name__ == "__main__":
    main()